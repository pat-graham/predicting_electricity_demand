---
title: "Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(smooth)
library(caret)
library(keras)
library(tseries)
```

# Project

```{r}

# Read in data. Data sourced from https://www.ferc.gov/docs-filing/forms/form-714/data.asp?fbclid=IwAR3edmimq4-j8_wzt51Sj-uaSkt8__BZE36opiitB3AlN_f5TahcYfeSPJs
data = read.csv("data/P3S2_hourly_clean.csv")
respondents = unique(data$respondent_id)

# Remove hour25
data = data[-32] # remove hour25

# Convert hourly data into list form. Note 12 years of data, with 3 gap years in 2008, 2012 and 2016.
unpacked_data = matrix(nrow = 12*365*24+3*24, ncol = length(respondents))
m = 1
n = 1
for (r in respondents)
{
  current_data = filter(data, respondent_id == r)
  
  for(j in 1:nrow(current_data))
  {
    for(i in 8:31)
    {
      unpacked_data[n,m] = current_data[j,i] 
      n = n + 1
    }
  }
  n = 1
  m  = m + 1
}

unpacked_data = as.data.frame(unpacked_data)

colnames(unpacked_data) = c("R101", "R118", "R157", "R160", "R171", "R172", "R180", "R182", "R194", "R197", "R210", "R211", "R232", "R233", "R234", "R235", "R236", "R240", "R243", "R251", "R253", "R263", "R267", "R275")
time_span =  seq(from = as.POSIXct("2006-1-1 0:00", tz="UTC"), to = as.POSIXct("2017-12-31 23:00", tz="UTC"), by="hour")  
rownames(unpacked_data) = time_span

# Respondent 253 does not have complete data, so remove
unpacked_data = select(unpacked_data, -R253)

aggregate_data = rowSums(unpacked_data/1e3) # data in Gigawatts

ggplot() + geom_line(aes(x = time_span, y = aggregate_data)) + xlab("Year") + ylab("Demand (GW)") + ggtitle("Aggregate US Electricity Demand") + theme(plot.title = element_text(hjust = 0.5))

### Investigating data
aggregate_data <- ts(aggregate_data, start = 2006-1-1, frequency = 24)
aggregate_data2 <- ts(diff(log(aggregate_data)), start = 2006-1-1, frequency = 24)#make data into time series for R
aggregate_components <- decompose(aggregate_data) #decompose the data
aggregate_components2 <- decompose(aggregate_data2)
plot(aggregate_components) # plot the decomposition
plot(aggregate_components2)

### Further investigation for ARIMA Modelling
adf.test(diff(log(aggregate_data)), alternative="stationary", k=0) #stationary enough for ARIMA Modelling (taking the log removed unequal variance, taking the difference removes trend) Note with Keras we can model that though
acf(diff(log(aggregate_data))) #still lags...
pacf(diff(log(aggregate_data))) #pcf is much better

#fitting a bad ARIMA model
(fit <- arima(diff(log(aggregate_data)), c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 24))) #need to do a grid search to find the best AIC for  (p,d,q)
pred <- predict(fit, n.ahead = 10*24)


# RULES
# test data 20% of training
# The test set should ideally be at least as large as the maximum forecast horizon required.

# USEFUL FUNCTIONS
# window() -> extraxcts data from a timepoint onwards
# subsetting() -> extracts a subset of a time sries
# ts() -> turns into time series data
# autoplot() -> plots time series data
# ggAcf -> autocorrelation

```
