---
title: "Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(forecast)
library(caret)
```

# Project

```{r}

# Read in data. Data sourced from https://www.ferc.gov/docs-filing/forms/form-714/data.asp?fbclid=IwAR3edmimq4-j8_wzt51Sj-uaSkt8__BZE36opiitB3AlN_f5TahcYfeSPJs
data = read.csv("data/P3S2_hourly_clean.csv")
respondents = unique(data$respondent_id)
timezones = unique(data$timezone)

# Remove hour25
data = data[-32] # remove hour25
#data$timezone[seq(65746, 66110,1)] = filter(data, respondent_id == 235)$timezone[400] 

# Convert hourly data into list form. Note 12 years of data, with 3 gap years in 2008, 2012 and 2016.
unpacked_data = matrix(nrow = 12*365*24+3*24, ncol = length(respondents))
timezones = matrix(nrow = 12*365*24+3*24, ncol = length(respondents))
m = 1
n = 1
for (r in respondents)
{
  current_data = filter(data, respondent_id == r)

  for(j in 1:nrow(current_data))
  {
    for(i in 8:31)
    {
      unpacked_data[n,m] = current_data[j,i] 
      timezones[n,m] = as.character(current_data$timezone[j])
      n = n + 1
    }
  }
  n = 1
  m  = m + 1
}

# Convert to data frame
unpacked_data = unpacked_data[,-21] # Respondent 253 has NAs at end
timezones = timezones[,-21]
unpacked_data = unpacked_data[,-16] # Respondent 235 has 0's at start
timezones = timezones[,-16]
unpacked_data = unpacked_data[,-15] # Respondent 234 has corrupt timezone data
timezones = timezones[,-15]
unpacked_data = timezones[,-12] # Respondent 211 206 timezone? 
timezones = timezones[,-12]

unpacked_data = as.data.frame(unpacked_data)

# Rename rows and columns
colnames(unpacked_data) = c("R101", "R118", "R157", "R160", "R171", "R172", "R180", "R182", "R194", "R197", "R210", "R232", "R233", "R236", "R240", "R243", "R251", "R263", "R267", "R275")
colnames(timezones) = c("R101", "R118", "R157", "R160", "R171", "R172", "R180", "R182", "R194", "R197", "R210", "R232", "R233", "R236", "R240", "R243", "R251", "R263", "R267", "R275")
time_span =  seq(from = as.POSIXct("2006-1-1 0:00", tz="UTC"), to = as.POSIXct("2017-12-31 23:00", tz="UTC"), by="hour")  
rownames(unpacked_data) = time_span

# Compute aggregate electricity demand
aggregate_demand = rowSums(unpacked_data/1e3) # data in Gigawatts

# ============================================== TIME SERIES MODEL ============================================== 

# Split into training and test
offset = 24*31
train = seq(1/3*length(aggregate_demand) - offset, 1/3*length(aggregate_demand), by = 1)
test = seq(1/3*length(aggregate_demand), 1/3*length(aggregate_demand) + offset, by = 1)

# fit1 time series benchmark model
fit1 = auto.arima(aggregate_demand[train], d = 1)
fit2 = ets(aggregate_demand[train], model = "ANN")

# Forecast on test set
demand_forecast1 = forecast(fit1, h = length(test))
demand_forecast2 = forecast(fit2, h = length(test))

ggplot() + geom_line(aes(x = time_span[test], y = demand_forecast1$mean, color = "forecast")) + geom_line(aes(x = time_span[train], y = aggregate_demand[train], color = "train")) + geom_line(aes(x = time_span[test], y = demand_forecast1$upper[,2], color = "upper")) + geom_line(aes(x = time_span[test], y = demand_forecast1$lower[,2], color = "lower")) 

ggplot() + geom_line(aes(x = time_span[test], y = demand_forecast2$mean, color = "forecast")) + geom_line(aes(x = time_span[train], y = aggregate_demand[train], color = "train")) + geom_line(aes(x = time_span[test], y = demand_forecast2$upper[,2], color = "upper")) + geom_line(aes(x = time_span[test], y = demand_forecast2$lower[,2], color = "lower")) 

ggplot() + geom_line(aes(x = time_span[train], y = aggregate_demand[train], color = "train"))+ geom_line(aes(x = time_span[test], y = aggregate_demand[test], color = "actual")) 

# Need to figure out why this is trash ^

# ============================================== SEQUENTIAL NEURAL NETWORK ============================================== 

# Code here takes inpsiration from https://keras.rstudio.com/articles/tutorial_basic_regression.html

# Training/test split + network memory
split = 2/3
winSize = 24 # 1 days memory
split_point = length(aggregate_demand)*split # last point where training data has a labelled prediction

# Manually Generate Training Data indexes
train_size = length(aggregate_demand)*split - winSize
train_data_index = matrix(ncol = winSize, nrow = train_size)
train_label_index = vector(mode = "numeric", length = train_size)
for (i in seq(from = 1, to = train_size, by = 1))
{
  train_data_index[i,] = seq(from = i, to = i + winSize-1, by = 1)
  train_label_index[i] = i + winSize
}

# Manual Generate test data indexes
offset = length(aggregate_demand)*split - winSize
test_size = length(aggregate_demand)*(1 - split) - 1
test_data_index = matrix(ncol = winSize, nrow = test_size)
test_label_index = vector(mode = "numeric", length = test_size)
for (i in seq(from = 1, to = test_size, by = 1))
{
  test_data_index[i,] = seq(from = offset + i, to = offset + i + winSize-1, by = 1)
  test_label_index[i] = offset + i + winSize
}

# Apply to create train part of split
iter = length(train_label_index)
train_data = matrix(ncol = winSize, nrow = iter)
train_label = vector(mode = "numeric", length = iter)
for (i in seq(from = 1, to = iter, by = 1))
{
  train_data[i,] = aggregate_demand[train_data_index[i,]]
  train_label[i] = aggregate_demand[train_label_index[i]]
}

# Apply to create test part of split
iter = length(test_label_index)
test_data = matrix(ncol = winSize, nrow = iter)
test_label = vector(mode = "numeric", length = iter)
for (i in seq(from = 1, to = iter, by = 1))
{
  test_data[i,] = aggregate_demand[test_data_index[i,]]
  test_label[i] = aggregate_demand[test_label_index[i]]
}

# Standardise input data for neural network
train_data <- scale(train_data) 
col_means_train <- attr(train_data, "scaled:center") 
col_stddevs_train <- attr(train_data, "scaled:scale")
test_data <- scale(test_data, center = col_means_train, scale = col_stddevs_train)

# Create neural network
model = keras_model_sequential() 
model %>%
    layer_dense(units = 64, activation = "relu",
                input_shape = winSize) %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 1)
  
model %>% compile(
    loss = "mse",
    optimizer = optimizer_rmsprop(),
    metrics = list("mean_absolute_error")
  )

# Train neural network
history = model %>% fit(
  train_data, train_label,
  epochs = 20, 
  validation_split = 0.2,
  verbose = 0
)

# Plot the model fitting performance
plot(history, metrics = "mean_absolute_error", smooth = FALSE)
  
# predict model on test set
test_preds = model %>% predict(test_data)

# Plot results on large scale
ggplot() + geom_line(aes(x = time_span[test_label_index], y = test_preds, color = "forecast")) + geom_line(aes(x = time_span[train_label_index], y = aggregate_demand[train_label_index], color = "train"))  + geom_line(aes(x = time_span[test_label_index], y = test_label, color = "actual")) + xlab("Year") + ylab("Electricity Demand (GW)") + ggtitle("Results of Neural Network Forecast")

# Define an inspection interval for graph close to split point
look_back = 24*7
look_forward = 24*16 #24 hours * number of days
ggplot() + 
  geom_line(aes(x = time_span[tail(train_label_index,look_back)], y = tail(train_label,look_back),  colour = "train")) +
  geom_line(aes(x = time_span[head(test_label_index,look_forward)], y =  head(test_label, look_forward), colour = "actual")) +
  geom_line(aes(x = time_span[head(test_label_index,look_forward)], y = head(test_preds, look_forward), colour = "predictions" )) + 
  xlab("Month") + ylab("Electricity Demand (GW)") + ggtitle("Results of Neural Network Forecast - Close Up")


```
