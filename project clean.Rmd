---
title: "project clean"
author: "James Louis Nguyen 27839397"
date: "10 October 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(fable)
library(tsibble)
library(keras)
library(tensorflow)

```

```{r}
P3S2_hourly_clean <- read.csv("data/P3S2_hourly_clean.csv")
data <- P3S2_hourly_clean
respondents = unique(data$respondent_id)
```

```{r}
test <- test %>% filter(respondent_id == 101) %>% select(everything(), -hour25)
test <- test %>% gather(key, load, -c(respondent_id:timezone), -c(eia_code:dow))
```


```{r}
test_101 <- test %>% filter(key == "hour01")

test_101 %>% ggplot(aes(x=plan_date, y=load, colour=as.factor(report_yr))) + geom_line(group=1) + 
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

test_101[1,]
```


```{r}
test %>% ggplot(aes(x=plan_date, y=load, colour=as.factor(report_yr))) + geom_line(group=1) + facet_wrap(~respondent_id)
```


```{r}
#cheers will
# Remove hour25
data = data[-32] # remove hour25

# Convert hourly data into list form. Note 12 years of data, with 3 gap years in 2008, 2012 and 2016.
unpacked_data = matrix(nrow = 12*365*24+3*24, ncol = length(respondents))
m = 1
n = 1
for (r in respondents)
{
  current_data = filter(data, respondent_id == r)
  
  for(j in 1:nrow(current_data))
  {
    for(i in 8:31)
    {
      unpacked_data[n,m] = current_data[j,i] 
      n = n + 1
    }
  }
  n = 1
  m  = m + 1
}

unpacked_data = as.data.frame(unpacked_data)

colnames(unpacked_data) = c("R101", "R118", "R157", "R160", "R171", "R172", "R180", "R182", "R194", "R197", "R210", "R211", "R232", "R233", "R234", "R235", "R236", "R240", "R243", "R251", "R253", "R263", "R267", "R275")
time_span =  seq(from = as.POSIXct("2006-1-1 0:00", tz="UTC"), to = as.POSIXct("2017-12-31 23:00", tz="UTC"), by="hour")  
rownames(unpacked_data) = time_span

# Respondent 253 does not have complete data, so remove
unpacked_data = select(unpacked_data, -R253)

aggregate_data = rowSums(unpacked_data/1e3) # data in Gigawatts

```

```{r}
aggregate_data <- data.frame(aggregate_data)
aggregate_data <- rownames_to_column(aggregate_data, "Time")
aggregate_data <- aggregate_data %>% mutate(Time = as.Date(Time))

aggregate_data_h1 <- aggregate_data[seq(1,nrow(aggregate_data),24),] %>% as_tsibble()
autoplot(aggregate_data_h1)

arima_h1 <- aggregate_data_h1 %>% model(ARIMA(aggregate_data, stepwise=FALSE, approximation=FALSE))
report(arima_h1)

arima_h1_fc <- arima_h1 %>% forecast(h=365)

arima_h1_fc %>% autoplot(aggregate_data_h1)

```

```{r}
aggregate_data_og <- aggregate_data


aggregate_data <- data.frame(load=aggregate_data_og)
aggregate_data <- rownames_to_column(aggregate_data, "Time")
aggregate_data <- aggregate_data %>% mutate(Time = as.POSIXct(Time,format="%Y-%m-%d %H:%M:%S",tz="Etc/GMT+8"))

#aggregate_data %>% filter(is.na(Time))


aggregate_data <- aggregate_data %>% as_tsibble()

#autoplot(aggregate_data)

#model <- aggregate_data %>% model(ARIMA(load))
#report(model)

#model_fc <- model %>% forecast(h=8760)

#model_fc %>% autoplot(aggregate_data)
```

```{r}
generator <- function(data, lookback, delay, min_index, max_index,
                      shuffle = FALSE, batch_size = 128, step = 6) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
    }

    samples <- array(0, dim = c(length(rows),
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
                      
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]]-1,
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay,2]
    }           
    list(samples, targets)
  }
}
```

```{r}
loadvalue <- aggregate_data$load %>% as.matrix()

lookback <- 8760
step <- 24
delay <- 8760
batch_size <- 256

train_gen <- generator(
  loadvalue,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 100000,
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)

val_gen = generator(
  loadvalue,
  lookback = lookback,
  delay = delay,
  min_index = 100001,
  max_index = 150000,
  step = step,
  batch_size = batch_size
)

test_gen <- generator(
  loadvalue,
  lookback = lookback,
  delay = delay,
  min_index = 150001,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)
```

```{r}
train <- loadvalue[1:70128,] %>% as.matrix()
val <- loadvalue[70129:87661,] %>% as.matrix()
test <- loadvalue[87662:105192,] %>% as.matrix()

val_steps <- 8760
test_steps <- (nrow(data) - 150001 - lookback) / batch_size
```


```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 32, input_shape = NULL) %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1)


model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)


history <- model %>% fit(
  train,
  steps_per_epoch = 500,
  epochs = 20,
  validation_data = val,
  batch_size = 32
)
```


```{r}

```

